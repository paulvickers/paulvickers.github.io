<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://paulvickers.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://paulvickers.github.io/" rel="alternate" type="text/html" /><updated>2017-12-06T12:50:14+00:00</updated><id>https://paulvickers.github.io/</id><title type="html">Paul Vickers</title><subtitle>Sonification, auditory display, human-computer interaction, aesthetics, network security, visualization, digital living, computational perceptualization.</subtitle><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><entry><title type="html">Sonification for Structural Biology and Structure-based Drug Design</title><link href="https://paulvickers.github.io/chat/public/Sonification-for-Structural-Biology/" rel="alternate" type="text/html" title="Sonification for Structural Biology and Structure-based Drug Design" /><published>2017-12-06T00:00:00+00:00</published><updated>2017-12-06T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Sonification%20for%20Structural%20Biology</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-for-Structural-Biology/">&lt;p&gt;The visualisation of structural biology data can be quite challenging as the datasets
are complex, in particular the intrinsic dynamics/flexibility. Therefore some
researchers have looked into the use of sonification for the display of proteins.
 You can find more about the project on the &lt;a href=&quot;/chison&quot;&gt;project page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">The visualisation of structural biology data can be quite challenging as the datasets are complex, in particular the intrinsic dynamics/flexibility. Therefore some researchers have looked into the use of sonification for the display of proteins.</summary></entry><entry><title type="html">DSSon pre-print</title><link href="https://paulvickers.github.io/chat/public/Direct-segmented-sonification-preprint/" rel="alternate" type="text/html" title="DSSon pre-print" /><published>2017-12-05T00:00:00+00:00</published><updated>2017-12-05T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Direct-segmented-sonification-preprint</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Direct-segmented-sonification-preprint/">&lt;p&gt;I have posted a &lt;a href=&quot;http://arxiv.org/abs/1711.11368&quot;&gt;pre-print on arXiv&lt;/a&gt; of the work Robert Höldrich and I developed on
direct segmented sonification. You can find more about the project on the &lt;a href=&quot;/DSSon&quot;&gt;DSSon page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">I have posted a [pre-print on arXiv](http://arxiv.org/abs/1711.11368) of our direct segmented sonification project.</summary></entry><entry><title type="html">Sonification Repositories</title><link href="https://paulvickers.github.io/chat/public/Sonification-repositories/" rel="alternate" type="text/html" title="Sonification Repositories" /><published>2017-12-01T00:00:00+00:00</published><updated>2017-12-01T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Sonification-repositories</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-repositories/">&lt;p&gt;This site provides links to our sonification research project repositories on Github. These repos
contain source code, data sets, and example sonifications and each has a DOI to make it
citable. The ones available so far are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/DSSon&quot;&gt;nuson-DSSon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/alchemical_sensing/&quot;&gt;nuson-moson&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/socs/&quot;&gt;nuson-socs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/SoNSTAR&quot;&gt;nuson-SoNSTAR&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">This site provides links to our sonification research project repositories.</summary></entry><entry><title type="html">DSSon: Direct Segmented Sonification</title><link href="https://paulvickers.github.io/chat/public/Direct-Segmented-Sonification/" rel="alternate" type="text/html" title="DSSon: Direct Segmented Sonification" /><published>2017-10-05T00:00:00+01:00</published><updated>2017-10-05T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Direct-Segmented-Sonification</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Direct-Segmented-Sonification/">&lt;p&gt;Following a research visit I made in the spring of this year to the 
&lt;a href=&quot;http://iem.kug.ac.at/en/institute-of-electronic-music-and-acoustics.html&quot;&gt;Institute of Electronic Music and Acoustics&lt;/a&gt;
 at the Kunst Uni Graz, &lt;a href=&quot;http://iem.kug.ac.at/en/people.html?tx_kugpeople_pi1%5Bperson_nr%5D=50114&amp;amp;cHash=eb4d7486e953326e239071165ea47ccf&quot;&gt;Robert Höldrich&lt;/a&gt;
 and I have been collaborating on a new sonification technique
that picks out regions of interest in a signal while preserving the original temporal regime. Called
Direct Segmented Sonification, it leverages the directness of audification while using
a modified auditory graphing approach to sonify segments of the original data signal
while preserving the original temporal regime. Read more on the &lt;a href=&quot;/DSSon&quot;&gt;DSSon project page&lt;/a&gt;. From there you will find a link to the project’s 
github repository which contains source code and audio examples. A paper on the technique 
is currently under review.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">Robert Höldrich and I have been collaborating on a new sonification technique that picks out regions of interest in a signal while preserving the original temporal regime.</summary></entry><entry><title type="html">soniFRED: Sonification of Biomechanical Exercise Data</title><link href="https://paulvickers.github.io/chat/public/Sonification-of-FRED-exercise/" rel="alternate" type="text/html" title="soniFRED: Sonification of Biomechanical Exercise Data" /><published>2017-06-02T00:00:00+01:00</published><updated>2017-06-02T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Sonification-of-FRED-exercise</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-of-FRED-exercise/">&lt;p&gt;The &lt;a href=&quot;https://paulvickers.github.io/SoniFRED/&quot;&gt;soniFRED project&lt;/a&gt; is a piece of 
collaborative research between Northumbria University’s Department of Computer and 
Information Sciences and the &lt;a href=&quot;http://www.aerospacemed.rehab/&quot;&gt;Aerospace Medicine and Rehabilitation Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We spent a week at the &lt;a href=&quot;https://www.life.org.uk/&quot;&gt;Centre for Life&lt;/a&gt;
testing different information presentation methods to help people with low back pain
exercise most effectively with the Functional Re-adaptive Exercise Device (FRED). Combinations
of visual and auditory feedback (including two sonification schemes) were given to 
participants and the data will be analysed to see what ways of giving the users feedback
about how they are using the machine lead to improved performance.&lt;/p&gt;

&lt;p&gt;You can get a sense of what we were doing by watching this &lt;a href=&quot;https://vimeo.com/219668024&quot;&gt;short video&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">We spent a week at the [Centre for Life](https://www.life.org.uk/) testing different information presentation methods to help people with low back pain exercise most effectively.</summary></entry><entry><title type="html">Displays: Special issue on the sonification of real-time data</title><link href="https://paulvickers.github.io/chat/public/Special-issue/" rel="alternate" type="text/html" title="Displays: Special issue on the sonification of real-time data" /><published>2017-04-14T00:00:00+01:00</published><updated>2017-04-14T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Special-issue</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Special-issue/">&lt;p&gt;David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the 
sonification of real-time data. This special issue presents five articles drawn from
across the range of sonification practice but which all focus on how to communicate
information about real-time data.&lt;/p&gt;

&lt;p&gt;The contents are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216302244&quot;&gt;Preface to the Special Issue on Sonification&lt;/a&gt;” by Paul Vickers, David Worrall, and Richard So.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300634&quot;&gt;Comparative study on the effect of Parameter Mapping Sonification on perceived instabilities, efficiency, and accuracy in real-time interactive exploration of noisy data streams&lt;/a&gt;” by David Poirier-Quinot, Gaetan Parseihian, and Brian F.G. Katz.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300646&quot;&gt;Sonification of a network’s self-organized criticality for real-time situational awareness&lt;/a&gt;” by Paul Vickers, Chris Laing, and Tom Fairfax.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300828&quot;&gt;Towards a systematic approach to real-time sonification design for surface electromyography&lt;/a&gt;” by S. Camille Peres, Daniel Verona, Tariq Nisar, and Paul Ritchey.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301652&quot;&gt;The sound of smile: Auditory biofeedback of facial EMG activity&lt;/a&gt;” by Yuki Nakayama, Yuji Takano, Masaki Matsubara, Kenji Suzuki, and Hiroko Terasawa.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301949&quot;&gt;EcoSonic: Auditory peripheral monitoring of fuel consumption for fuel-efficient driving&lt;/a&gt;” by Jan Hammerschmidt and Thomas Hermann.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can see the full issue at &lt;a href=&quot;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&quot;&gt;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the sonification of real-time data.</summary></entry><entry><title type="html">Body, Sound and Space in Music and Beyond: Multimodal Explorations</title><link href="https://paulvickers.github.io/chat/public/Body-Sound-and-Space-in-Music-and-Beyond/" rel="alternate" type="text/html" title="Body, Sound and Space in Music and Beyond: Multimodal Explorations" /><published>2017-04-13T00:00:00+01:00</published><updated>2017-04-13T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Body-Sound-and-Space-in-Music-and-Beyond</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Body-Sound-and-Space-in-Music-and-Beyond/">&lt;p&gt;Clemens Wollner has edited a terrific volume entitled &lt;a href=&quot;https://www.routledge.com/Body-Sound-and-Space-in-Music-and-Beyond-Multimodal-Explorations/Wollner/p/book/9781472485403&quot;&gt;&lt;em&gt;Body, Sound and Space in Music and Beyond: Multimodal Explorations&lt;/em&gt;&lt;/a&gt;
which has been published by Routledge. With Bennett Hogg and David Worrall, I co-wrote Chapter 6,
&lt;em&gt;Aesthetics of sonification: Taking the subject-position&lt;/em&gt; which takes forward my
work on sonification aesthetics and proposes a way of considering sonification design that
may help to bring the listener’s past experience into the listening experience.&lt;/p&gt;

&lt;p&gt;Here’s the blurb about the book from its website:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Body and space refer to vital and interrelated dimensions in the experience of sounds
and music. Sounds have an overwhelming impact on feelings of bodily presence and 
inform us about the space we experience. Even in situations where visual information 
is artificial or blurred, such as in virtual environments or certain genres of film 
and computer games, sounds may shape our perceptions and lead to surprising new 
experiences. This book discusses recent developments in a range of interdisciplinary 
fields, taking into account the rapidly changing ways of experiencing sounds and 
music, the consequences for how we engage with sonic events in daily life and the 
technological advancements that offer insights into state-of-the-art methods and 
future perspectives. Topics range from the pleasures of being locked into the beat 
of the music, perception–action coupling and bodily resonance, and affordances of 
musical instruments, to neural processing and cross-modal experiences of space and 
pitch. Applications of these findings are discussed for movement sonification, room 
acoustics, networked performance, and for the spatial coordination of movements in 
dance, computer gaming and interactive artistic installations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can find details about the book &lt;a href=&quot;https://www.routledge.com/Body-Sound-and-Space-in-Music-and-Beyond-Multimodal-Explorations/Wollner/p/book/9781472485403&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="music" /><summary type="html">Body and space refer to vital and interrelated dimensions in the experience of sounds and music. Sounds have an overwhelming impact on feelings of bodily presence and inform us about the space we experience.</summary></entry><entry><title type="html">Audiogram: Embed audio in social media!</title><link href="https://paulvickers.github.io/chat/public/Audiogram-embed-audio-in-social-media-copy/" rel="alternate" type="text/html" title="Audiogram: Embed audio in social media!" /><published>2017-01-19T00:00:00+00:00</published><updated>2017-01-19T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Audiogram-embed-audio-in-social-media%20copy</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Audiogram-embed-audio-in-social-media-copy/">&lt;p&gt;We know as sonification researchers that social media platforms prefer video over audio. This
is frustrating as we want to be able, say, to embed sonification examples in our Tweets.&lt;/p&gt;

&lt;p&gt;Well, now we can thanks to New York Public radio who have created an open source tool to 
convert audio into video which can be embedded in social media. The tool is called
&lt;a href=&quot;https://medium.com/@WNYC/socialaudio-e648e8a5f2e9#.xi3wf2nst&quot;&gt;Audiogram&lt;/a&gt; and you can get it from their &lt;a href=&quot;https://github.com/nypublicradio/audiogram&quot;&gt;git repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/DrPaulVickers/status/822066017283215361&quot;&gt;Here’s a Tweet&lt;/a&gt; with it in action.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="tools" /><summary type="html">New York Public radio have created an open source tool to convert audio into video which can be embedded in social media.</summary></entry><entry><title type="html">Sonification of protein folding</title><link href="https://paulvickers.github.io/chat/public/sonification-of-protein-folding/" rel="alternate" type="text/html" title="Sonification of protein folding" /><published>2016-10-21T00:00:00+01:00</published><updated>2016-10-21T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/sonification-of-protein-folding</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/sonification-of-protein-folding/">&lt;p&gt;From &lt;a href=&quot;http://www.newsweek.com/new-technique-allows-scientists-listen-proteins-511840&quot;&gt;Newsweek&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Researchers have come up with a bold new method for representing and understanding a protein’s shape: translating it into music.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ordinarily, to show the structure of proteins—found in every cell of every living thing—scientists create visual representations made of loops and folds and sheets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Read the full story on &lt;a href=&quot;http://www.newsweek.com/new-technique-allows-scientists-listen-proteins-511840&quot;&gt;Newsweek’s site&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="embodied computing" /><summary type="html">'Researchers have come up with a bold new method for representing and understanding a protein’s shape: translating it into music.'</summary></entry><entry><title type="html">Paul on the Sound and Data podcast</title><link href="https://paulvickers.github.io/chat/public/sound-and-data-podcast/" rel="alternate" type="text/html" title="Paul on the Sound and Data podcast" /><published>2016-07-25T00:00:00+01:00</published><updated>2016-07-25T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/sound-and-data-podcast</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/sound-and-data-podcast/">&lt;p&gt;A little while I was interviewed by &lt;a href=&quot;http://creativedisturbance.org/people/scot-gresham-lancaster/&quot;&gt;Scot Gresham-Lancaster&lt;/a&gt; for his Sound and Data 
sonification podcast. The &lt;a href=&quot;http://creativedisturbance.org/podcast/dr-paul-vickers-sonification-ethical-computing-and-standup-comedy-eng/&quot;&gt;interview is now live&lt;/a&gt;
so I encourage you to listen to it. All feedback welcome.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="ethical computing" /><category term="embodied computing" /><summary type="html">A short discussion with Dr. Vickers about his approach to sonification including some discussion of his recent standup routine regarding sonification as its main topic.</summary></entry></feed>