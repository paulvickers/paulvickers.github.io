<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-10-05T13:34:39+01:00</updated><id>http://localhost:4000/</id><title type="html">Paul Vickers</title><subtitle>Sonification, auditory display, human-computer interaction, aesthetics, network security, visualization, digital living, computational perceptualization.</subtitle><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><entry><title type="html">DSSon: Direct Segmented Sonification</title><link href="http://localhost:4000/chat/public/Direct-Segmented-Sonification/" rel="alternate" type="text/html" title="DSSon: Direct Segmented Sonification" /><published>2017-10-05T00:00:00+01:00</published><updated>2017-10-05T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Direct-Segmented-Sonification</id><content type="html" xml:base="http://localhost:4000/chat/public/Direct-Segmented-Sonification/">&lt;p&gt;Following a research visit I made in the spring of this year to the 
&lt;a href=&quot;http://iem.kug.ac.at/en/institute-of-electronic-music-and-acoustics.html&quot;&gt;Institute of Electronic Music and Acoustics&lt;/a&gt;
 at the Kunst Uni Graz, &lt;a href=&quot;http://iem.kug.ac.at/en/people.html?tx_kugpeople_pi1%5Bperson_nr%5D=50114&amp;amp;cHash=eb4d7486e953326e239071165ea47ccf&quot;&gt;Robert Höldrich&lt;/a&gt;
 and I have been collaborating on a new sonification technique
that picks out regions of interest in a signal while preserving the original temporal regime. Called
Direct Segmented Sonification, it leverages the directness of audification while using
a modified auditory graphing approach to sonify segments of the original data signal
while preserving the original temporal regime. Read more on the &lt;a href=&quot;/DSSon&quot;&gt;DSSon project page&lt;/a&gt;. From there you will find a link to the project’s 
github repository which contains source code and audio examples. A paper on the technique 
is currently under review.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">Robert Höldrich and I have been collaborating on a new sonification technique that picks out regions of interest in a signal while preserving the original temporal regime.</summary></entry><entry><title type="html">soniFRED: Sonification of Biomechanical Exercise Data</title><link href="http://localhost:4000/chat/public/Sonification-of-FRED-exercise/" rel="alternate" type="text/html" title="soniFRED: Sonification of Biomechanical Exercise Data" /><published>2017-06-02T00:00:00+01:00</published><updated>2017-06-02T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Sonification-of-FRED-exercise</id><content type="html" xml:base="http://localhost:4000/chat/public/Sonification-of-FRED-exercise/">&lt;p&gt;The &lt;a href=&quot;https://paulvickers.github.io/SoniFRED/&quot;&gt;soniFRED project&lt;/a&gt; is a piece of 
collaborative research between Northumbria University’s Department of Computer and 
Information Sciences and the &lt;a href=&quot;http://www.aerospacemed.rehab/&quot;&gt;Aerospace Medicine and Rehabilitation Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We spent a week at the &lt;a href=&quot;https://www.life.org.uk/&quot;&gt;Centre for Life&lt;/a&gt;
testing different information presentation methods to help people with low back pain
exercise most effectively with the Functional Re-adaptive Exercise Device (FRED). Combinations
of visual and auditory feedback (including two sonification schemes) were given to 
participants and the data will be analysed to see what ways of giving the users feedback
about how they are using the machine lead to improved performance.&lt;/p&gt;

&lt;p&gt;You can get a sense of what we were doing by watching this &lt;a href=&quot;https://vimeo.com/219668024&quot;&gt;short video&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">We spent a week at the [Centre for Life](https://www.life.org.uk/) testing different information presentation methods to help people with low back pain exercise most effectively.</summary></entry><entry><title type="html">Displays: Special issue on the sonification of real-time data</title><link href="http://localhost:4000/chat/public/Special-issue/" rel="alternate" type="text/html" title="Displays: Special issue on the sonification of real-time data" /><published>2017-04-14T00:00:00+01:00</published><updated>2017-04-14T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Special-issue</id><content type="html" xml:base="http://localhost:4000/chat/public/Special-issue/">&lt;p&gt;David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the 
sonification of real-time data. This special issue presents five articles drawn from
across the range of sonification practice but which all focus on how to communicate
information about real-time data.&lt;/p&gt;

&lt;p&gt;The contents are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216302244&quot;&gt;Preface to the Special Issue on Sonification&lt;/a&gt;” by Paul Vickers, David Worrall, and Richard So.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300634&quot;&gt;Comparative study on the effect of Parameter Mapping Sonification on perceived instabilities, efficiency, and accuracy in real-time interactive exploration of noisy data streams&lt;/a&gt;” by David Poirier-Quinot, Gaetan Parseihian, and Brian F.G. Katz.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300646&quot;&gt;Sonification of a network’s self-organized criticality for real-time situational awareness&lt;/a&gt;” by Paul Vickers, Chris Laing, and Tom Fairfax.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300828&quot;&gt;Towards a systematic approach to real-time sonification design for surface electromyography&lt;/a&gt;” by S. Camille Peres, Daniel Verona, Tariq Nisar, and Paul Ritchey.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301652&quot;&gt;The sound of smile: Auditory biofeedback of facial EMG activity&lt;/a&gt;” by Yuki Nakayama, Yuji Takano, Masaki Matsubara, Kenji Suzuki, and Hiroko Terasawa.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301949&quot;&gt;EcoSonic: Auditory peripheral monitoring of fuel consumption for fuel-efficient driving&lt;/a&gt;” by Jan Hammerschmidt and Thomas Hermann.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can see the full issue at &lt;a href=&quot;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&quot;&gt;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the sonification of real-time data.</summary></entry><entry><title type="html">Body, Sound and Space in Music and Beyond: Multimodal Explorations</title><link href="http://localhost:4000/chat/public/Body-Sound-and-Space-in-Music-and-Beyond/" rel="alternate" type="text/html" title="Body, Sound and Space in Music and Beyond: Multimodal Explorations" /><published>2017-04-13T00:00:00+01:00</published><updated>2017-04-13T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Body-Sound-and-Space-in-Music-and-Beyond</id><content type="html" xml:base="http://localhost:4000/chat/public/Body-Sound-and-Space-in-Music-and-Beyond/">&lt;p&gt;Clemens Wollner has edited a terrific volume entitled &lt;a href=&quot;https://www.routledge.com/Body-Sound-and-Space-in-Music-and-Beyond-Multimodal-Explorations/Wollner/p/book/9781472485403&quot;&gt;&lt;em&gt;Body, Sound and Space in Music and Beyond: Multimodal Explorations&lt;/em&gt;&lt;/a&gt;
which has been published by Routledge. With Bennett Hogg and David Worrall, I co-wrote Chapter 6,
&lt;em&gt;Aesthetics of sonification: Taking the subject-position&lt;/em&gt; which takes forward my
work on sonification aesthetics and proposes a way of considering sonification design that
may help to bring the listener’s past experience into the listening experience.&lt;/p&gt;

&lt;p&gt;Here’s the blurb about the book from its website:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Body and space refer to vital and interrelated dimensions in the experience of sounds
and music. Sounds have an overwhelming impact on feelings of bodily presence and 
inform us about the space we experience. Even in situations where visual information 
is artificial or blurred, such as in virtual environments or certain genres of film 
and computer games, sounds may shape our perceptions and lead to surprising new 
experiences. This book discusses recent developments in a range of interdisciplinary 
fields, taking into account the rapidly changing ways of experiencing sounds and 
music, the consequences for how we engage with sonic events in daily life and the 
technological advancements that offer insights into state-of-the-art methods and 
future perspectives. Topics range from the pleasures of being locked into the beat 
of the music, perception–action coupling and bodily resonance, and affordances of 
musical instruments, to neural processing and cross-modal experiences of space and 
pitch. Applications of these findings are discussed for movement sonification, room 
acoustics, networked performance, and for the spatial coordination of movements in 
dance, computer gaming and interactive artistic installations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can find details about the book &lt;a href=&quot;https://www.routledge.com/Body-Sound-and-Space-in-Music-and-Beyond-Multimodal-Explorations/Wollner/p/book/9781472485403&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="music" /><summary type="html">Body and space refer to vital and interrelated dimensions in the experience of sounds and music. Sounds have an overwhelming impact on feelings of bodily presence and inform us about the space we experience.</summary></entry><entry><title type="html">Audiogram: Embed audio in social media!</title><link href="http://localhost:4000/chat/public/Audiogram-embed-audio-in-social-media-copy/" rel="alternate" type="text/html" title="Audiogram: Embed audio in social media!" /><published>2017-01-19T00:00:00+00:00</published><updated>2017-01-19T00:00:00+00:00</updated><id>http://localhost:4000/chat/public/Audiogram-embed-audio-in-social-media%20copy</id><content type="html" xml:base="http://localhost:4000/chat/public/Audiogram-embed-audio-in-social-media-copy/">&lt;p&gt;We know as sonification researchers that social media platforms prefer video over audio. This
is frustrating as we want to be able, say, to embed sonification examples in our Tweets.&lt;/p&gt;

&lt;p&gt;Well, now we can thanks to New York Public radio who have created an open source tool to 
convert audio into video which can be embedded in social media. The tool is called
&lt;a href=&quot;https://medium.com/@WNYC/socialaudio-e648e8a5f2e9#.xi3wf2nst&quot;&gt;Audiogram&lt;/a&gt; and you can get it from their &lt;a href=&quot;https://github.com/nypublicradio/audiogram&quot;&gt;git repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/DrPaulVickers/status/822066017283215361&quot;&gt;Here’s a Tweet&lt;/a&gt; with it in action.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="tools" /><summary type="html">New York Public radio have created an open source tool to convert audio into video which can be embedded in social media.</summary></entry><entry><title type="html">Sonification of protein folding</title><link href="http://localhost:4000/chat/public/sonification-of-protein-folding/" rel="alternate" type="text/html" title="Sonification of protein folding" /><published>2016-10-21T00:00:00+01:00</published><updated>2016-10-21T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/sonification-of-protein-folding</id><content type="html" xml:base="http://localhost:4000/chat/public/sonification-of-protein-folding/">&lt;p&gt;From &lt;a href=&quot;http://www.newsweek.com/new-technique-allows-scientists-listen-proteins-511840&quot;&gt;Newsweek&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Researchers have come up with a bold new method for representing and understanding a protein’s shape: translating it into music.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ordinarily, to show the structure of proteins—found in every cell of every living thing—scientists create visual representations made of loops and folds and sheets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Read the full story on &lt;a href=&quot;http://www.newsweek.com/new-technique-allows-scientists-listen-proteins-511840&quot;&gt;Newsweek’s site&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="embodied computing" /><summary type="html">'Researchers have come up with a bold new method for representing and understanding a protein’s shape: translating it into music.'</summary></entry><entry><title type="html">Paul on the Sound and Data podcast</title><link href="http://localhost:4000/chat/public/sound-and-data-podcast/" rel="alternate" type="text/html" title="Paul on the Sound and Data podcast" /><published>2016-07-25T00:00:00+01:00</published><updated>2016-07-25T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/sound-and-data-podcast</id><content type="html" xml:base="http://localhost:4000/chat/public/sound-and-data-podcast/">&lt;p&gt;A little while I was interviewed by &lt;a href=&quot;http://creativedisturbance.org/people/scot-gresham-lancaster/&quot;&gt;Scot Gresham-Lancaster&lt;/a&gt; for his Sound and Data 
sonification podcast. The &lt;a href=&quot;http://creativedisturbance.org/podcast/dr-paul-vickers-sonification-ethical-computing-and-standup-comedy-eng/&quot;&gt;interview is now live&lt;/a&gt;
so I encourage you to listen to it. All feedback welcome.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="ethical computing" /><category term="embodied computing" /><summary type="html">A short discussion with Dr. Vickers about his approach to sonification including some discussion of his recent standup routine regarding sonification as its main topic.</summary></entry><entry><title type="html">Routledge Companion to Sounding Art</title><link href="http://localhost:4000/news/public/sounding-art/" rel="alternate" type="text/html" title="Routledge Companion to Sounding Art" /><published>2016-07-06T00:00:00+01:00</published><updated>2016-07-06T00:00:00+01:00</updated><id>http://localhost:4000/news/public/sounding-art</id><content type="html" xml:base="http://localhost:4000/news/public/sounding-art/">&lt;blockquote&gt;
  &lt;p&gt;The &lt;a href=&quot;https://www.routledge.com/The-Routledge-Companion-to-Sounding-Art/Cobussen-Meelberg-Truax/p/book/9781138780613&quot;&gt;Routledge Companion to Sounding Art&lt;/a&gt;
presents an overview of the issues, methods, and approaches crucial for the study of sound in artistic practice. Thirty-six essays cover a variety of interdisciplinary approaches to studying sounding art from the fields of musicology, cultural studies, sound design, auditory culture, art history, and philosophy. The companion website hosts sound examples and links to further resources.&lt;/p&gt;

  &lt;p&gt;The collection is organized around six main themes:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Sounding Art: The notion of sounding art, its relation to sound studies, and its evolution and possibilities.&lt;/li&gt;
    &lt;li&gt;Acoustic Knowledge and Communication: How we approach, study, and analyze sound and the challenges of writing about sound.&lt;/li&gt;
    &lt;li&gt;Listening and Memory: Listening from different perspectives, from the psychology of listening to embodied and technologically mediated listening.&lt;/li&gt;
    &lt;li&gt;Acoustic Spaces, Identities and Communities: How humans arrange their sonic environments, how this relates to sonic identity, how music contributes to our environment, and the ethical and political implications of sound.&lt;/li&gt;
    &lt;li&gt;Sound Technologies and Media: The impact of sonic technologies on contemporary culture, electroacoustic innovation, and how the way we make and access music has changed.&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;With contributions from leading scholars and cutting-edge researchers, The Routledge Companion to Sounding Art is an essential resource for anyone studying the intersection of sound and art.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">The Routledge Companion to Sounding Art presents an overview of the issues, methods, and approaches crucial for the study of sound in artistic practice.</summary></entry><entry><title type="html">Gang-gang. Beanie-counters add a new kind of beanie</title><link href="http://localhost:4000/news/public/beanies/" rel="alternate" type="text/html" title="Gang-gang. Beanie-counters add a new kind of beanie" /><published>2016-07-06T00:00:00+01:00</published><updated>2016-07-06T00:00:00+01:00</updated><id>http://localhost:4000/news/public/beanies</id><content type="html" xml:base="http://localhost:4000/news/public/beanies/">&lt;p&gt;From the &lt;a href=&quot;http://www.canberratimes.com.au/act-news/canberra-life/ganggang-beaniecounters-add-a-new-kind-of-beanie-20160705-gpynvo.html&quot;&gt;Canberra Times&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Canberrans know that we are in midwinter, beanie-demanding, ear-protecting times. This very morning your columnist has come to work wearing with pride the swish navy blue and gold CBR Brave ice-hockey supporters’ beanie.&lt;/p&gt;

  &lt;p&gt;We don’t know how this treasured headwear was made but we are sure that its manufacture won’t compare with the ingenuity with which the beanies have been made to be handed out, at registration, to participants in this week’s annual International Conference on Auditory Display (ICAD 2016).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Read the full story on the &lt;a href=&quot;http://www.canberratimes.com.au/act-news/canberra-life/ganggang-beaniecounters-add-a-new-kind-of-beanie-20160705-gpynvo.html&quot;&gt;newspaper’s site&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">From the Canberra Times:</summary></entry><entry><title type="html">ICAD 2016</title><link href="http://localhost:4000/news/public/ICAD-2016/" rel="alternate" type="text/html" title="ICAD 2016" /><published>2016-06-23T00:00:00+01:00</published><updated>2016-06-23T00:00:00+01:00</updated><id>http://localhost:4000/news/public/ICAD-2016</id><content type="html" xml:base="http://localhost:4000/news/public/ICAD-2016/">&lt;p&gt;This year’s &lt;a href=&quot;http://icad.org/icad2016/&quot;&gt;International conference on Auditory Display (ICAD)&lt;/a&gt;
 is meeting in Canberra, Australia from 3-7 July. We’ve got a great programme lined up so 
 come along and hear what all the fuss is about. :-)&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">The International Conference on Auditory Display returns to Oz</summary></entry></feed>