<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="https://paulvickers.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://paulvickers.github.io/" rel="alternate" type="text/html" /><updated>2018-05-22T14:34:49+01:00</updated><id>https://paulvickers.github.io/</id><title type="html">Paul Vickers</title><subtitle>Sonification, auditory display, human-computer interaction, aesthetics, network security, visualization, digital living, computational perceptualization.</subtitle><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><entry><title type="html">Sonification of network traffic flow for monitoring and situational awareness</title><link href="https://paulvickers.github.io/chat/public/Network-sonification/" rel="alternate" type="text/html" title="Sonification of network traffic flow for monitoring and situational awareness" /><published>2018-04-20T00:00:00+01:00</published><updated>2018-04-20T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Network-sonification</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Network-sonification/">&lt;p&gt;Maintaining situational awareness of what is happening within a computer network is challenging, not only because the behaviour happens within machines, but also because data traffic speeds and volumes are beyond human ability to process. Visualisation techniques are widely used to present information about network traffic dynamics. Although they provide operators with an overall view and specific information about particular traffic or attacks on the network, they often still fail to represent the events in an understandable way. Also, because they require visual attention they are not well suited to continuous monitoring scenarios in which network administrators must carry out other tasks.&lt;/p&gt;

&lt;p&gt;As part of his PhD research &lt;a href=&quot;https://paulvickers.github.io/SoNSTAR/&quot;&gt;Mohamed Debashi&lt;/a&gt; has built the SoNSTAR (Sonification of Networks for SiTuational AwaReness) tool. SoNSTAR is a real-time sonification system for monitoring computer networks to support network administrators’ situational awareness. SoNSTAR provides an auditory representation of all the TCP/IP traffic within a network based on the different traffic flows between between network hosts. A user study showed that SoNSTAR raises situational awareness levels by enabling operators to understand network behaviour and with the benefit of lower workload demands (as measured by the NASA TLX method) than visual techniques. SoNSTAR identifies network traffic features by inspecting the status flags of TCP/IP packet headers. Combinations of these features define particular traffic events which are mapped to recorded sounds to generate a soundscape that represents the real-time status of the network traffic environment. The sequence, timing, and loudness of the different sounds allow the network to be monitored and anomalous behaviour to be detected without the need to continuously watch a monitor screen.&lt;/p&gt;

&lt;p&gt;You can read the whole story in our &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0195948&quot;&gt;PLoS One article&lt;/a&gt;&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="network" /><category term="situation awareness" /><category term="cyber security" /><summary type="html">Maintaining situational awareness of what is happening within a computer network is challenging, not only because the behaviour happens within machines, but also because data traffic speeds and volumes are beyond human ability to process.</summary></entry><entry><title type="html">Medical Image Sonification for Detecting Cancer</title><link href="https://paulvickers.github.io/chat/public/Sonification-of-cancer/" rel="alternate" type="text/html" title="Medical Image Sonification for Detecting Cancer" /><published>2018-04-19T00:00:00+01:00</published><updated>2018-04-19T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Sonification-of-cancer</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-of-cancer/">&lt;p&gt;Here’s an interesting short story in &lt;a href=&quot;https://www.scientificamerican.com/article/detecting-cancer-by-sound-audio1/&quot;&gt;Scientific American&lt;/a&gt; about the sonification
of medical images for helping clinicians to detect cancerous cells.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="medical imaging" /><category term="cancer" /><summary type="html">Here's an interesting short story in Scientific American about the sonification of medical images for helping clinicians to detect cancerous cells.</summary></entry><entry><title type="html">Sonification of Network Traffic Flow for Monitoring and Situational Awareness</title><link href="https://paulvickers.github.io/chat/public/SoNSTAR/" rel="alternate" type="text/html" title="Sonification of Network Traffic Flow for Monitoring and Situational Awareness" /><published>2017-12-20T00:00:00+00:00</published><updated>2017-12-20T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/SoNSTAR</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/SoNSTAR/">&lt;p&gt;Maintaining situational awareness of what is happening within a computer network is
challenging, not least because the behaviour happens within computers and communications
networks, but also because data traffic speeds and volumes are beyond human ability to
process. Visualisation techniques are widely used to present information about the
dynamics of network traffic dynamics. Although they provide operators with an overall
view and specific information about particular traffic or attacks on the network, they
often still fail to represent the events in an understandable way. Also, visualisations
require visual attention and so are not well suited to continuous monitoring scenarios
in which network administrators must carry out other tasks. Situational awareness is
critical and essential for decision-making in the domain of computer network monitoring
where it is vital to be able to identify and recognize network environment
behaviours.Here we present SoNSTAR (Sonification of Networks for SiTuational AwaReness),
a real-time sonification system to be used in the monitoring of computer networks to
support the situational awareness of network administrators. SoNSTAR provides an
auditory representation of all the TCP/IP protocol traffic within a network based on the
different traffic flows between between network hosts. SoNSTAR narrows the gap between
network administrators and the cyber environment so they can more quickly recognise and
learn about the way the traffic flows within their network behave and change. SoNSTAR
raises situational awareness levels for computer network defence by allowing operators
to achieve better understanding and performance while imposing less workload compared to
visual techniques. SoNSTAR identifyies the features of network traffic flows by
inspecting the status flags of TCP/IP packet headers. Different combinations of these
features define particular traffic events and these these events are mapped to recorded
sounds to generate a soundscape that represents the real-time status of the network
traffic environment. Listening to the sequence, timing, and loudness of the different
sounds within the soundscape allows the network administrator to monitor the network and
recognise anomalous behaviour quickly and without having to continuously look at a
computer screen.&lt;/p&gt;

&lt;p&gt;A pre-print of the article is &lt;a href=&quot;http://arxiv.org/abs/1712.07029&quot;&gt;available on arXiv&lt;/a&gt;. It is 
part of the &lt;a href=&quot;/SoNSTAR/&quot;&gt;SoNSTAR&lt;/a&gt; project.&lt;/p&gt;

&lt;ul class=&quot;bibliography&quot; reversed=&quot;&quot;&gt;&lt;li&gt;&lt;span id=&quot;Debashi:2018&quot;&gt;Debashi, M., &amp;amp; Vickers, P. (2018). &lt;b&gt;Sonification of Network Traffic Flow for Monitoring and Situational Awareness&lt;/b&gt;. &lt;i&gt;PLoS One&lt;/i&gt;, &lt;i&gt;13&lt;/i&gt;(4), 1–31.&lt;/span&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-md-2&quot;&gt;
		&lt;div class=&quot;btn-group&quot;&gt;
			&lt;button type=&quot;button&quot; class=&quot;btn btn-info btn-xs dropdown-toggle&quot; data-toggle=&quot;dropdown&quot;&gt;
				Bib Entry
				&lt;span class=&quot;caret&quot;&gt;&lt;/span&gt;
			&lt;/button&gt;
			&lt;ul class=&quot;dropdown-menu dropdown-menu-left&quot; role=&quot;menu&quot;&gt;
				&lt;li&gt;&lt;pre&gt;@article{Debashi:2018,
  author = {Debashi, Mohamed and Vickers, Paul},
  doi = {10.1371/journal.pone.0195948},
  journal = {PLoS One},
  month = &quot;19~&quot; # apr,
  number = {4},
  pages = {1--31},
  publisher = {Public Library of Science},
  title = {Sonification of Network Traffic Flow for Monitoring and Situational Awareness},
  volume = {13},
  year = {2018}
}
&lt;/pre&gt;&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/div&gt;
	&lt;/div&gt;
	&lt;div class=&quot;col-md-7&quot;&gt;
			
			
			
			
			
			
				&lt;a role=&quot;button&quot; class=&quot;btn btn-warning btn-xs&quot; href=&quot;https://doi.org/10.1371/journal.pone.0195948&quot; target=&quot;_blank&quot;&gt;doi&lt;/a&gt;
			
			
			
	&lt;/div&gt;
	&lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;/li&gt;&lt;/ul&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="network monitoring" /><category term="situational awareness" /><summary type="html">Maintaining situational awareness of what is happening within a computer network is challenging, not least because the behaviour happens within computers and communications networks, but also because data traffic speeds and volumes are beyond human ability to process.</summary></entry><entry><title type="html">graphTPP: A multivariate based method for interactive graph layout and analysis</title><link href="https://paulvickers.github.io/chat/public/graphTPP/" rel="alternate" type="text/html" title="graphTPP: A multivariate based method for interactive graph layout and analysis" /><published>2017-12-18T00:00:00+00:00</published><updated>2017-12-18T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/graphTPP</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/graphTPP/">&lt;p&gt;Graph layout is the process of creating a visual representation of a graph through a
node-link diagram. Node-attribute graphs have additional data stored on the nodes which
describe certain properties of the nodes called attributes. Typical force-directed
representations often produce hairball-like structures that neither aid in understanding
the graph’s topology nor the relationship to its attributes. The aim of this research
was to investigate the use of node-attributes for graph layout in order to improve the
analysis process and to give further insight into the graph over purely topological
layouts. In this article we present graphTPP, a graph based extension to targeted
projection pursuit (TPP) — an interactive, linear, dimension reduction technique — as a
method for graph layout and subsequent further analysis. TPP allows users to control the
projection and is optimised for clustering. Three case studies were conducted in the
areas of influence graphs, network security, and citation networks. In each case
graphTPP was shown to outperform standard force-directed techniques and even other
dimension reduction methods in terms of clarity of clustered structure in the layout,
the association between the structure and the attributes and the insights elicited in
each domain area. A pre-print of the article is &lt;a href=&quot;http://arxiv.org/abs/1712.05644&quot;&gt;available on arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;ul class=&quot;bibliography&quot; reversed=&quot;&quot;&gt;&lt;li&gt;&lt;span id=&quot;Gibson:2018&quot;&gt;Gibson, H., &amp;amp; Vickers, P. (2017). &lt;b&gt;graphTPP: A multivariate based method for interactive graph layout and analysis&lt;/b&gt;. &lt;i&gt;ArXiv Preprint ArXiv:1712.05644&lt;/i&gt;.&lt;/span&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-md-2&quot;&gt;
		&lt;div class=&quot;btn-group&quot;&gt;
			&lt;button type=&quot;button&quot; class=&quot;btn btn-info btn-xs dropdown-toggle&quot; data-toggle=&quot;dropdown&quot;&gt;
				Bib Entry
				&lt;span class=&quot;caret&quot;&gt;&lt;/span&gt;
			&lt;/button&gt;
			&lt;ul class=&quot;dropdown-menu dropdown-menu-left&quot; role=&quot;menu&quot;&gt;
				&lt;li&gt;&lt;pre&gt;@article{Gibson:2018,
  author = {Gibson, Helen and Vickers, Paul},
  journal = {arXiv preprint arXiv:1712.05644},
  month = dec,
  pdf = {https://arxiv.org/pdf/1712.05644},
  title = {graphTPP: A multivariate based method for interactive graph layout and analysis},
  year = {2017}
}
&lt;/pre&gt;&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/div&gt;
	&lt;/div&gt;
	&lt;div class=&quot;col-md-7&quot;&gt;
			
				&lt;a role=&quot;button&quot; class=&quot;btn btn-primary btn-xs&quot; href=&quot;https://arxiv.org/pdf/1712.05644&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;
			
			
			
			
			
			
			
			
	&lt;/div&gt;
	&lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;/li&gt;&lt;/ul&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="graph layout" /><category term="targeted projection pursuit" /><category term="adjacency matrix" /><summary type="html">Graph layout is the process of creating a visual representation of a graph through a node-link diagram. Node-attribute graphs have additional data stored on the nodes which describe certain properties of the nodes called attributes.</summary></entry><entry><title type="html">Sonification for Structural Biology and Structure-based Drug Design</title><link href="https://paulvickers.github.io/chat/public/Sonification-for-Structural-Biology/" rel="alternate" type="text/html" title="Sonification for Structural Biology and Structure-based Drug Design" /><published>2017-12-06T00:00:00+00:00</published><updated>2017-12-06T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Sonification%20for%20Structural%20Biology</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-for-Structural-Biology/">&lt;p&gt;The visualisation of structural biology data can be quite challenging as the datasets
are complex, in particular the intrinsic dynamics/flexibility. Therefore some
researchers have looked into the use of sonification for the display of proteins.
 You can find more about the project on the &lt;a href=&quot;/chison&quot;&gt;project page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">The visualisation of structural biology data can be quite challenging as the datasets are complex, in particular the intrinsic dynamics/flexibility. Therefore some researchers have looked into the use of sonification for the display of proteins.</summary></entry><entry><title type="html">DSSon pre-print</title><link href="https://paulvickers.github.io/chat/public/Direct-segmented-sonification-preprint/" rel="alternate" type="text/html" title="DSSon pre-print" /><published>2017-12-05T00:00:00+00:00</published><updated>2017-12-05T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Direct-segmented-sonification-preprint</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Direct-segmented-sonification-preprint/">&lt;p&gt;I have posted a &lt;a href=&quot;http://arxiv.org/abs/1711.11368&quot;&gt;pre-print on arXiv&lt;/a&gt; of the work Robert Höldrich and I developed on
direct segmented sonification. You can find more about the project on the &lt;a href=&quot;/DSSon&quot;&gt;DSSon page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">I have posted a [pre-print on arXiv](http://arxiv.org/abs/1711.11368) of our direct segmented sonification project.</summary></entry><entry><title type="html">Sonification Repositories</title><link href="https://paulvickers.github.io/chat/public/Sonification-repositories/" rel="alternate" type="text/html" title="Sonification Repositories" /><published>2017-12-01T00:00:00+00:00</published><updated>2017-12-01T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Sonification-repositories</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-repositories/">&lt;p&gt;This site provides links to our sonification research project repositories on Github. These repos
contain source code, data sets, and example sonifications and each has a DOI to make it
citable. The ones available so far are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/DSSon&quot;&gt;nuson-DSSon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/alchemical_sensing/&quot;&gt;nuson-moson&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/socs/&quot;&gt;nuson-socs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/SoNSTAR&quot;&gt;nuson-SoNSTAR&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">This site provides links to our sonification research project repositories.</summary></entry><entry><title type="html">DSSon: Direct Segmented Sonification</title><link href="https://paulvickers.github.io/chat/public/Direct-Segmented-Sonification/" rel="alternate" type="text/html" title="DSSon: Direct Segmented Sonification" /><published>2017-10-05T00:00:00+01:00</published><updated>2017-10-05T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Direct-Segmented-Sonification</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Direct-Segmented-Sonification/">&lt;p&gt;Following a research visit I made in the spring of this year to the 
&lt;a href=&quot;http://iem.kug.ac.at/en/institute-of-electronic-music-and-acoustics.html&quot;&gt;Institute of Electronic Music and Acoustics&lt;/a&gt;
 at the Kunst Uni Graz, &lt;a href=&quot;http://iem.kug.ac.at/en/people.html?tx_kugpeople_pi1%5Bperson_nr%5D=50114&amp;amp;cHash=eb4d7486e953326e239071165ea47ccf&quot;&gt;Robert Höldrich&lt;/a&gt;
 and I have been collaborating on a new sonification technique
that picks out regions of interest in a signal while preserving the original temporal regime. Called
Direct Segmented Sonification, it leverages the directness of audification while using
a modified auditory graphing approach to sonify segments of the original data signal
while preserving the original temporal regime. Read more on the &lt;a href=&quot;/DSSon&quot;&gt;DSSon project page&lt;/a&gt;. From there you will find a link to the project’s 
github repository which contains source code and audio examples. A paper on the technique 
is currently under review.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">Robert Höldrich and I have been collaborating on a new sonification technique that picks out regions of interest in a signal while preserving the original temporal regime.</summary></entry><entry><title type="html">soniFRED: Sonification of Biomechanical Exercise Data</title><link href="https://paulvickers.github.io/chat/public/Sonification-of-FRED-exercise/" rel="alternate" type="text/html" title="soniFRED: Sonification of Biomechanical Exercise Data" /><published>2017-06-02T00:00:00+01:00</published><updated>2017-06-02T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Sonification-of-FRED-exercise</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-of-FRED-exercise/">&lt;p&gt;The &lt;a href=&quot;https://paulvickers.github.io/SoniFRED/&quot;&gt;soniFRED project&lt;/a&gt; is a piece of 
collaborative research between Northumbria University’s Department of Computer and 
Information Sciences and the &lt;a href=&quot;http://www.aerospacemed.rehab/&quot;&gt;Aerospace Medicine and Rehabilitation Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We spent a week at the &lt;a href=&quot;https://www.life.org.uk/&quot;&gt;Centre for Life&lt;/a&gt;
testing different information presentation methods to help people with low back pain
exercise most effectively with the Functional Re-adaptive Exercise Device (FRED). Combinations
of visual and auditory feedback (including two sonification schemes) were given to 
participants and the data will be analysed to see what ways of giving the users feedback
about how they are using the machine lead to improved performance.&lt;/p&gt;

&lt;p&gt;You can get a sense of what we were doing by watching this &lt;a href=&quot;https://vimeo.com/219668024&quot;&gt;short video&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">We spent a week at the [Centre for Life](https://www.life.org.uk/) testing different information presentation methods to help people with low back pain exercise most effectively.</summary></entry><entry><title type="html">Displays: Special issue on the sonification of real-time data</title><link href="https://paulvickers.github.io/chat/public/Special-issue/" rel="alternate" type="text/html" title="Displays: Special issue on the sonification of real-time data" /><published>2017-04-14T00:00:00+01:00</published><updated>2017-04-14T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Special-issue</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Special-issue/">&lt;p&gt;David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the 
sonification of real-time data. This special issue presents five articles drawn from
across the range of sonification practice but which all focus on how to communicate
information about real-time data.&lt;/p&gt;

&lt;p&gt;The contents are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216302244&quot;&gt;Preface to the Special Issue on Sonification&lt;/a&gt;” by Paul Vickers, David Worrall, and Richard So.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300634&quot;&gt;Comparative study on the effect of Parameter Mapping Sonification on perceived instabilities, efficiency, and accuracy in real-time interactive exploration of noisy data streams&lt;/a&gt;” by David Poirier-Quinot, Gaetan Parseihian, and Brian F.G. Katz.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300646&quot;&gt;Sonification of a network’s self-organized criticality for real-time situational awareness&lt;/a&gt;” by Paul Vickers, Chris Laing, and Tom Fairfax.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300828&quot;&gt;Towards a systematic approach to real-time sonification design for surface electromyography&lt;/a&gt;” by S. Camille Peres, Daniel Verona, Tariq Nisar, and Paul Ritchey.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301652&quot;&gt;The sound of smile: Auditory biofeedback of facial EMG activity&lt;/a&gt;” by Yuki Nakayama, Yuji Takano, Masaki Matsubara, Kenji Suzuki, and Hiroko Terasawa.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301949&quot;&gt;EcoSonic: Auditory peripheral monitoring of fuel consumption for fuel-efficient driving&lt;/a&gt;” by Jan Hammerschmidt and Thomas Hermann.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can see the full issue at &lt;a href=&quot;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&quot;&gt;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the sonification of real-time data.</summary></entry></feed>