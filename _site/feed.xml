<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://paulvickers.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://paulvickers.github.io/" rel="alternate" type="text/html" /><updated>2020-07-22T17:08:00+01:00</updated><id>https://paulvickers.github.io/feed.xml</id><title type="html">Paul Vickers</title><subtitle>Sonification, auditory display, human-computer interaction, aesthetics, network security, visualization, digital living, computational perceptualization.</subtitle><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><entry><title type="html">A Radical New Interdisciplinary Space for Sonification</title><link href="https://paulvickers.github.io/chat/public/radical/" rel="alternate" type="text/html" title="A Radical New Interdisciplinary Space for Sonification" /><published>2020-07-22T00:00:00+01:00</published><updated>2020-07-22T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/radical</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/radical/">&lt;h2 id=&quot;radical-a-radical-new-interdisciplinary-space-for-sonification&quot;&gt;RADICAL: A Radical New Interdisciplinary Space for Sonification&lt;/h2&gt;
&lt;p&gt;Department of Computer and Information Sciences
Northumbria University, Newcastle-upon-Tyne, UK&lt;/p&gt;

&lt;p&gt;I am pleased to announce a forthcoming research project to develop a fundamental understanding of the relationship between sonification design and the listener and to stimulate a revitalised agenda for sonification research and practice.&lt;/p&gt;

&lt;p&gt;Project RADICAL is funded by a three-year Research Project Grant by the Leverhulme Trust (RPG-2020-113) and will commence in October 2020.&lt;/p&gt;

&lt;p&gt;Bringing together researchers in sonification, music practice, listening, aesthetics, and ethnography at Northumbria University and Newcastle University, and using Northumbria’s &lt;a href=&quot;https://paulvickers.github.io/iko/&quot;&gt;IKO 3D loudspeaker&lt;/a&gt; RADICAL will create a new space for sonification research.&lt;/p&gt;

&lt;p&gt;For further information about the project and about two new post-doctoral research posts that we are recruiting to, please see the &lt;a href=&quot;/radical/&quot;&gt;project RADICAL page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="SPS" /><category term="sonification" /><category term="auditory display" /><category term="space" /><summary type="html">RADICAL is a new research project funded by the Leverhulme Trust..</summary></entry><entry><title type="html">Shared Perceptual Spaces in the Sciences and the Arts</title><link href="https://paulvickers.github.io/chat/public/SPS-Workshop/" rel="alternate" type="text/html" title="Shared Perceptual Spaces in the Sciences and the Arts" /><published>2019-11-11T00:00:00+00:00</published><updated>2019-11-11T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/SPS-Workshop</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/SPS-Workshop/">&lt;h2 id=&quot;shared-perceptual-spaces-in-the-sciences-and-the-arts&quot;&gt;Shared Perceptual Spaces in the Sciences and the Arts&lt;/h2&gt;
&lt;p&gt;Department of Computer and Information Sciences
Northumbria University, Newcastle-upon-Tyne, UK&lt;/p&gt;

&lt;p&gt;13 December, 2019&lt;/p&gt;

&lt;p&gt;Does your work cause you to think about the space around you and how a better understanding of the spaces we inhabit and the acoustic and aesthetic properties of those spaces could lead to improvements in the way you think about your own practice?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/IKO.jpg&quot; alt=&quot;image-left&quot; class=&quot;align-left&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On Friday 13th December, the Department of Computer and Information Sciences at Northumbria University, Newcastle upon Tyne, UK  will be hosting a 1-day cross-disciplinary workshop, lecture and concert.  This will be of interest to people involved in understanding how the space around us impacts us in our daily lives. The workshop will be led by Paul Vickers (&lt;a href=&quot;https://paulvickers.github.io/&quot;&gt;Northumbria&lt;/a&gt;), Gerriet, K. Sharma (&lt;a href=&quot;https://www.gksh.net/&quot;&gt;Graz, Austria&lt;/a&gt;), and Angela McArthur (&lt;a href=&quot;http://angelamcarthur.com/&quot;&gt;Queen Mary, University of London&lt;/a&gt;). It follows on from the popular workshop on sonification and sonic interaction design for space run by Paul and Gerriet at &lt;a href=&quot;http://angelamcarthur.com/soundstack-2019/&quot;&gt;Soundstack 2019&lt;/a&gt; on 8 November 2019.&lt;/p&gt;

&lt;p&gt;The workshop will feature the &lt;a href=&quot;https://iko.sonible.com/en.html&quot;&gt;IKO icosahedral loudspeaker&lt;/a&gt; which generates a stunning 3D sound field. Northumbria University has the only IKO in the UK, so this will be a great opportunity to come and see what IKO can do and the collaborative research opportunities it offers.&lt;/p&gt;

&lt;iframe width=&quot;480&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/khTSr_ZJB5M&quot; frameborder=&quot;1&quot;&gt; &lt;/iframe&gt;

&lt;h2 id=&quot;workshop-description&quot;&gt;WORKSHOP DESCRIPTION&lt;/h2&gt;

&lt;p&gt;In the broad field of electroacoustic composition and sound design we are dealing for some time now with spatial sound phenomena that not only come from a direction and head for a vanishing point in the concert or studio space. Rather these phenomena have spatial dimensions like proliferation, width, height etc. forming diverse sound masses that can penetrate, layer, move around each other and define by their properties — space itself. Thus, these phenomena are perceived by composers, scientists and audiences causing ‘something’ we call a shared perceptual space (SPS).&lt;/p&gt;

&lt;p&gt;Spatial composition has become a subject to academic curricula, workshops and master classes internationally. It is constantly triggering the development and extension of commercial and academic software solutions for the projection, placement and movement of phantom sources, the reproduction of higher order recordings of “natural” sound fields as well as the creation of so called immersive virtual sound environments.&lt;/p&gt;

&lt;p&gt;Moreover, as spatial computer music matures and consolidates within institutions and organizations, it is increasingly involving so-called 3D audio systems which can create auditory virtual environments (AVEs). Quite likely in the very near future AVEs will be part of many people’s everyday life, e.g. in cars, working spaces, intelligent homes, concert halls and computer games.&lt;/p&gt;

&lt;p&gt;For the composer, the question arises as to what extent a communicable or self-explanatory composition of plastic sound objects is conceptually, theoretically and at all practically possible when faced with changing architectural space situations, different cultural spatial descriptions, and perceptions. It is therefore a matter of finding parameters for an intersubjective space for the perception of three-dimensional sound phenomena. Is there within the field of space-sound composition a space at the place of the music, where the composer’s perception in the compositional process overlaps with both the engineers’ and audience’s perception? Can at least an approximate circumference of an overlap be described? How and from which sides (linguistic, technical, artistic, etc.) can this field be approached?&lt;/p&gt;

&lt;p&gt;In this cross-disciplinary workshop and lecture we are going to investigate different uses of the term SPS in a variety of fields as aesthetic strategies, showing that space has become one of the key aspects in all kinds of scientific and artistic, applied and theoretical disciplines. By discussing examples from music, musicology, sociology, philosophy, architecture and linguistics we are trying to extract variables that can help to formulate a perception based framework for a hybrid model of sound as space.&lt;/p&gt;

&lt;p&gt;The workshop is free to attend but space will be strictly limited. Therefore, if you would like to come please email paul[dot]vickers[at]northumbria.ac.uk by 20 November and supply the following information:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Name&lt;/li&gt;
  &lt;li&gt;Position&lt;/li&gt;
  &lt;li&gt;Affiliation institution (and department if a university)&lt;/li&gt;
  &lt;li&gt;Main field of research/scholarship/practice&lt;/li&gt;
  &lt;li&gt;Your reason for wishing to attend the workshop&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="SPS" /><category term="sonification" /><category term="auditory display" /><category term="space" /><summary type="html">1-day cross-disciplinary workshop, lecture and concert.</summary></entry><entry><title type="html">ICAD 2019: Call for Papers</title><link href="https://paulvickers.github.io/chat/public/ICAD-2019-CfP/" rel="alternate" type="text/html" title="ICAD 2019: Call for Papers" /><published>2018-10-02T00:00:00+01:00</published><updated>2018-10-02T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/ICAD-2019-CfP</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/ICAD-2019-CfP/">&lt;h2 id=&quot;icad-2019--call-for-submission-of-papers-and-extended-abstracts&quot;&gt;ICAD 2019 — Call for Submission of Papers and Extended Abstracts&lt;/h2&gt;
&lt;p&gt;25th International Conference on Auditory Display
Northumbria University, Newcastle-upon-Tyne, UK&lt;/p&gt;

&lt;p&gt;23–27 June, 2019&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://icad2019.icad.org&quot;&gt;https://icad2019.icad.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/ICAD2019&quot;&gt;https://twitter.com/ICAD2019&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Theme/Special focus of ICAD 2019: Sonification for Everyday Life.&lt;/p&gt;

&lt;p&gt;Digital technology and artificial intelligence are becoming embedded in the objects all around us, from consumer products to the built environment. Everyday life happens where People, Technology, and Place intersect. Our activities and movements are increasingly sensed, digitised and tracked. Of course, the data generated by modern life is a hugely important resource not just for companies who use it for commercial purposes, but it can also be harnessed for the benefit of the individuals it concerns. Sonification research that has hit the news headlines in recent times has often been related to big science done at large publicly funded labs with little impact on the day-to-day lives of people. At ICAD 2019 we want to explore how auditory display technologies and techniques may be used to enhance our everyday lives. From giving people access to what’s going on inside their own bodies, to the human concerns of living in a modern networked and technological city, the range of opportunities for auditory display is wide. The ICAD 2019 committee is seeking papers and extended abstracts that will contribute to knowledge of how sonification can support everyday life.&lt;/p&gt;

&lt;p&gt;Important Dates:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Friday 15th March 2019 - Deadline for submission of full papers.&lt;/li&gt;
  &lt;li&gt;Friday 22nd March 2019 - Deadline for submission of extended abstracts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For details on topics of interest, proposal format, submission instructions, and additional conference information please visit https://icad2019.icad.org/call-for-participation/&lt;/p&gt;

&lt;p&gt;Papers Chair:
Tony Stockman
&lt;a href=&quot;mailto:icad2019papers@icad.org&quot;&gt;icad2019papers@icad.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Conference Chairs:
Paul Vickers and Matti Gröhn
&lt;a href=&quot;mailto:icad2019chairs@icad.org&quot;&gt;icad2019chairs@icad.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;About ICAD:
First held in 1992, ICAD is a highly interdisciplinary conference with relevance to researchers, practitioners, artists, and graduate students working with sound to convey and explore information. The conference is unique in its specific focus on auditory displays and the range of interdisciplinary issues related to their use. Like its predecessors, ICAD 2019 will be a single-track conference, open to all, with no membership or affiliation requirements.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="ICAD" /><category term="sonification" /><category term="auditory display" /><summary type="html">Call for Submission of Papers and Extended Abstracts.</summary></entry><entry><title type="html">ICAD 2019: The 25th International Conference on Auditory Display</title><link href="https://paulvickers.github.io/chat/public/ICAD-2019/" rel="alternate" type="text/html" title="ICAD 2019: The 25th International Conference on Auditory Display" /><published>2018-08-25T00:00:00+01:00</published><updated>2018-08-25T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/ICAD-2019</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/ICAD-2019/">&lt;h2 id=&quot;icad-2019-sonification-for-everyday-life&quot;&gt;ICAD 2019: Sonification for Everyday Life&lt;/h2&gt;

&lt;p&gt;It is a pleasure to announce &lt;a href=&quot;https://icad2019.icad.org&quot;&gt;ICAD 2019, the 25th International Conference on Auditory Display&lt;/a&gt;. The conference is hosted by the &lt;a href=&quot;https://www.northumbria.ac.uk/about-us/academic-departments/computer-and-information-sciences/&quot;&gt;Department of Computer and Information Sciences, Northumbria University&lt;/a&gt; and will take place in Newcastle upon Tyne, UK on 23-27 June, 2019. The graduate student Think Tank (doctoral consortium) will be on Sunday, June 23, before the main conference.&lt;/p&gt;

&lt;p&gt;Digital technology and artificial intelligence are becoming embedded in the objects all around us, from consumer products to the built environment. Everyday life happens where People, Technology, and Place intersect. Our activities and movements are increasingly sensed, digitised and tracked. Of course, the data generated by modern life is a hugely important resource not just for companies who use it for commercial purposes, but it can also be harnessed for the benefit of the individuals it concerns.&lt;/p&gt;

&lt;p&gt;Sonification research that has hit the news headlines in recent times has often been related to big science done at large publicly funded labs with little impact on the day-to-day lives of people. At ICAD 2019 we want to explore how auditory display technologies and techniques may be used to enhance our everyday lives. From giving people access to what’s going on inside their own bodies, to the human concerns of living in a modern networked and technological city, the range of opportunities for auditory display is wide. The ICAD 2019 committee is seeking papers, extended abstracts, multimedia, concert pieces, demos, installations, workshops, and tutorials that will contribute to knowledge of how sonification can support everyday life.&lt;/p&gt;

&lt;p&gt;ICAD is a highly interdisciplinary academic conference with relevance to researchers, practitioners, musicians, and students interested in the design of sounds to support tasks, improve performance, guide decisions, augment awareness, and enhance experiences. It is unique in its singular focus on auditory displays and the array of perception, technology, and application areas that this encompasses. Like its predecessors, ICAD 2019 will be a single-track conference, open to all, with no membership or affiliation requirements.&lt;/p&gt;

&lt;p&gt;A full Call for Participation with details of the submission classes and dates will be posted on the &lt;a href=&quot;https://icad2019.icad.org&quot;&gt;conference website&lt;/a&gt; soon.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="ICAD" /><category term="sonification" /><category term="auditory display" /><summary type="html">It is a pleasure to announce ICAD 2019, the 25th International Conference on Auditory Display. The conference is hosted by the Department of Computer and Information Sciences, Northumbria University and will take place in Newcastle upon Tyne, UK on 23-27 June, 2019.</summary></entry><entry><title type="html">Sonification of Network Traffic for Detecting and Learning About Botnet Behavior</title><link href="https://paulvickers.github.io/chat/public/Sonification-and-Botnets-copy/" rel="alternate" type="text/html" title="Sonification of Network Traffic for Detecting and Learning About Botnet Behavior" /><published>2018-07-06T00:00:00+01:00</published><updated>2018-07-06T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Sonification-and-Botnets%20copy</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-and-Botnets-copy/">&lt;p&gt;By listening to a real-time sonification of their network over time (&lt;a href=&quot;https://paulvickers.github.io/chat/public/Network-sonification/&quot;&gt;see here&lt;/a&gt;), an operator is able to learn what their network typically sounds like. This led to the aural identification of suspicious behavior that had slipped through the Intrusion Detection System but which was, in fact, botnet activity.&lt;/p&gt;

&lt;p&gt;Today’s computer networks are under increasing threat from malicious activity. Botnets (networks of remotely controlled computers, or “bots”) operate in such a way that their activity superficially resembles normal network traffic which makes their behavior hard to detect by current intrusion detection systems (IDS). Therefore, new monitoring techniques are needed to enable network operators to detect botnet activity quickly and in real time. Here, we show a sonification technique using the SoNSTAR system that maps characteristics of network traffic to a real-time soundscape enabling an operator to hear and detect botnet activity.&lt;/p&gt;

&lt;p&gt;A case study demonstrated how using traffic log files alongside the interactive SoNSTAR system enabled the identification of new traffic patterns characteristic of botnet behavior and subsequently the effective targeting and real-time detection of botnet activity by a human operator. An experiment using
the 11.39 GiB ISOT botnet data set, containing labeled botnet traffic data, compared the SoNSTAR system with three leading machine learning-based traffic classifiers in a botnet activity detection test. SoNSTAR demonstrated greater accuracy (99.92%), precision (97.1%), and recall (99.5%) and much lower false
positive rates (0.007%) than the other techniques. The knowledge generated about characteristic botnet behaviors could be used in the development of future IDSs.&lt;/p&gt;

&lt;p&gt;You can read the whole story in our &lt;a href=&quot;https://doi.org/10.1109/ACCESS.2018.2847349&quot;&gt;IEEE Access article&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="network" /><category term="situation awareness" /><category term="cyber security" /><summary type="html">By listening to a real-time sonification of their network over time, an operator is able to learn what their network typically sounds like. This led to the aural identification of suspicious behavior that had slipped through the Intrusion Detection System but which was, in fact, botnet activity.</summary></entry><entry><title type="html">Sonification of network traffic flow for monitoring and situational awareness</title><link href="https://paulvickers.github.io/chat/public/Network-sonification/" rel="alternate" type="text/html" title="Sonification of network traffic flow for monitoring and situational awareness" /><published>2018-04-20T00:00:00+01:00</published><updated>2018-04-20T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Network-sonification</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Network-sonification/">&lt;p&gt;Maintaining situational awareness of what is happening within a computer network is challenging, not only because the behaviour happens within machines, but also because data traffic speeds and volumes are beyond human ability to process. Visualisation techniques are widely used to present information about network traffic dynamics. Although they provide operators with an overall view and specific information about particular traffic or attacks on the network, they often still fail to represent the events in an understandable way. Also, because they require visual attention they are not well suited to continuous monitoring scenarios in which network administrators must carry out other tasks.&lt;/p&gt;

&lt;p&gt;As part of his PhD research &lt;a href=&quot;https://paulvickers.github.io/SoNSTAR/&quot;&gt;Mohamed Debashi&lt;/a&gt; has built the SoNSTAR (Sonification of Networks for SiTuational AwaReness) tool. SoNSTAR is a real-time sonification system for monitoring computer networks to support network administrators’ situational awareness. SoNSTAR provides an auditory representation of all the TCP/IP traffic within a network based on the different traffic flows between between network hosts. A user study showed that SoNSTAR raises situational awareness levels by enabling operators to understand network behaviour and with the benefit of lower workload demands (as measured by the NASA TLX method) than visual techniques. SoNSTAR identifies network traffic features by inspecting the status flags of TCP/IP packet headers. Combinations of these features define particular traffic events which are mapped to recorded sounds to generate a soundscape that represents the real-time status of the network traffic environment. The sequence, timing, and loudness of the different sounds allow the network to be monitored and anomalous behaviour to be detected without the need to continuously watch a monitor screen.&lt;/p&gt;

&lt;p&gt;You can read the whole story in our &lt;a href=&quot;https://doi.org/10.1371/journal.pone.0195948&quot;&gt;PLoS One article&lt;/a&gt;&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="network" /><category term="situation awareness" /><category term="cyber security" /><summary type="html">Maintaining situational awareness of what is happening within a computer network is challenging, not only because the behaviour happens within machines, but also because data traffic speeds and volumes are beyond human ability to process.</summary></entry><entry><title type="html">Medical Image Sonification for Detecting Cancer</title><link href="https://paulvickers.github.io/chat/public/Sonification-of-cancer/" rel="alternate" type="text/html" title="Medical Image Sonification for Detecting Cancer" /><published>2018-04-19T00:00:00+01:00</published><updated>2018-04-19T00:00:00+01:00</updated><id>https://paulvickers.github.io/chat/public/Sonification-of-cancer</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-of-cancer/">&lt;p&gt;Here’s an interesting short story in &lt;a href=&quot;https://www.scientificamerican.com/article/detecting-cancer-by-sound-audio1/&quot;&gt;Scientific American&lt;/a&gt; about the sonification
of medical images for helping clinicians to detect cancerous cells.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="medical imaging" /><category term="cancer" /><summary type="html">Here's an interesting short story in Scientific American about the sonification of medical images for helping clinicians to detect cancerous cells.</summary></entry><entry><title type="html">Sonification of Network Traffic Flow for Monitoring and Situational Awareness</title><link href="https://paulvickers.github.io/chat/public/SoNSTAR/" rel="alternate" type="text/html" title="Sonification of Network Traffic Flow for Monitoring and Situational Awareness" /><published>2017-12-20T00:00:00+00:00</published><updated>2017-12-20T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/SoNSTAR</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/SoNSTAR/">&lt;p&gt;Maintaining situational awareness of what is happening within a computer network is
challenging, not least because the behaviour happens within computers and communications
networks, but also because data traffic speeds and volumes are beyond human ability to
process. Visualisation techniques are widely used to present information about the
dynamics of network traffic dynamics. Although they provide operators with an overall
view and specific information about particular traffic or attacks on the network, they
often still fail to represent the events in an understandable way. Also, visualisations
require visual attention and so are not well suited to continuous monitoring scenarios
in which network administrators must carry out other tasks. Situational awareness is
critical and essential for decision-making in the domain of computer network monitoring
where it is vital to be able to identify and recognize network environment
behaviours.Here we present SoNSTAR (Sonification of Networks for SiTuational AwaReness),
a real-time sonification system to be used in the monitoring of computer networks to
support the situational awareness of network administrators. SoNSTAR provides an
auditory representation of all the TCP/IP protocol traffic within a network based on the
different traffic flows between between network hosts. SoNSTAR narrows the gap between
network administrators and the cyber environment so they can more quickly recognise and
learn about the way the traffic flows within their network behave and change. SoNSTAR
raises situational awareness levels for computer network defence by allowing operators
to achieve better understanding and performance while imposing less workload compared to
visual techniques. SoNSTAR identifyies the features of network traffic flows by
inspecting the status flags of TCP/IP packet headers. Different combinations of these
features define particular traffic events and these these events are mapped to recorded
sounds to generate a soundscape that represents the real-time status of the network
traffic environment. Listening to the sequence, timing, and loudness of the different
sounds within the soundscape allows the network administrator to monitor the network and
recognise anomalous behaviour quickly and without having to continuously look at a
computer screen.&lt;/p&gt;

&lt;p&gt;A pre-print of the article is &lt;a href=&quot;http://arxiv.org/abs/1712.07029&quot;&gt;available on arXiv&lt;/a&gt;. It is 
part of the &lt;a href=&quot;/SoNSTAR/&quot;&gt;SoNSTAR&lt;/a&gt; project.&lt;/p&gt;

&lt;ul class=&quot;bibliography&quot; reversed=&quot;&quot;&gt;&lt;li&gt;&lt;span id=&quot;Debashi:2018&quot;&gt;Debashi, M., &amp;amp; Vickers, P. (2018). &lt;b&gt;Sonification of Network Traffic Flow for Monitoring and Situational Awareness&lt;/b&gt;. &lt;i&gt;PLoS One&lt;/i&gt;, &lt;i&gt;13&lt;/i&gt;(4), 1–31.&lt;/span&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-md-2&quot;&gt;
		&lt;div class=&quot;btn-group&quot;&gt;
			&lt;button type=&quot;button&quot; class=&quot;btn btn-info btn-xs dropdown-toggle&quot; data-toggle=&quot;dropdown&quot;&gt;
				Bib Entry
				&lt;span class=&quot;caret&quot;&gt;&lt;/span&gt;
			&lt;/button&gt;
			&lt;ul class=&quot;dropdown-menu dropdown-menu-left&quot; role=&quot;menu&quot;&gt;
				&lt;li&gt;&lt;pre&gt;@article{Debashi:2018,
  author = {Debashi, Mohamed and Vickers, Paul},
  doi = {10.1371/journal.pone.0195948},
  journal = {PLoS One},
  month = &quot;19~&quot; # apr,
  number = {4},
  pages = {1--31},
  publisher = {Public Library of Science},
  title = {Sonification of Network Traffic Flow for Monitoring and Situational Awareness},
  volume = {13},
  year = {2018}
}
&lt;/pre&gt;&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/div&gt;
	&lt;/div&gt;
	&lt;div class=&quot;col-md-7&quot;&gt;
			
			
			
			
			
			
				&lt;a role=&quot;button&quot; class=&quot;btn btn-warning btn-xs&quot; href=&quot;https://doi.org/10.1371/journal.pone.0195948&quot; target=&quot;_blank&quot;&gt;doi&lt;/a&gt;
			
			
			
	&lt;/div&gt;
	&lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;/li&gt;&lt;/ul&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="network monitoring" /><category term="situational awareness" /><summary type="html">Maintaining situational awareness of what is happening within a computer network is challenging, not least because the behaviour happens within computers and communications networks, but also because data traffic speeds and volumes are beyond human ability to process.</summary></entry><entry><title type="html">graphTPP: A multivariate based method for interactive graph layout and analysis</title><link href="https://paulvickers.github.io/chat/public/graphTPP/" rel="alternate" type="text/html" title="graphTPP: A multivariate based method for interactive graph layout and analysis" /><published>2017-12-18T00:00:00+00:00</published><updated>2017-12-18T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/graphTPP</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/graphTPP/">&lt;p&gt;Graph layout is the process of creating a visual representation of a graph through a
node-link diagram. Node-attribute graphs have additional data stored on the nodes which
describe certain properties of the nodes called attributes. Typical force-directed
representations often produce hairball-like structures that neither aid in understanding
the graph’s topology nor the relationship to its attributes. The aim of this research
was to investigate the use of node-attributes for graph layout in order to improve the
analysis process and to give further insight into the graph over purely topological
layouts. In this article we present graphTPP, a graph based extension to targeted
projection pursuit (TPP) — an interactive, linear, dimension reduction technique — as a
method for graph layout and subsequent further analysis. TPP allows users to control the
projection and is optimised for clustering. Three case studies were conducted in the
areas of influence graphs, network security, and citation networks. In each case
graphTPP was shown to outperform standard force-directed techniques and even other
dimension reduction methods in terms of clarity of clustered structure in the layout,
the association between the structure and the attributes and the insights elicited in
each domain area. A pre-print of the article is &lt;a href=&quot;http://arxiv.org/abs/1712.05644&quot;&gt;available on arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;ul class=&quot;bibliography&quot; reversed=&quot;&quot;&gt;&lt;li&gt;&lt;span id=&quot;Gibson:2018&quot;&gt;Gibson, H., &amp;amp; Vickers, P. (2017). &lt;b&gt;graphTPP: A multivariate based method for interactive graph layout and analysis&lt;/b&gt;. &lt;i&gt;ArXiv Preprint ArXiv:1712.05644&lt;/i&gt;.&lt;/span&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-md-2&quot;&gt;
		&lt;div class=&quot;btn-group&quot;&gt;
			&lt;button type=&quot;button&quot; class=&quot;btn btn-info btn-xs dropdown-toggle&quot; data-toggle=&quot;dropdown&quot;&gt;
				Bib Entry
				&lt;span class=&quot;caret&quot;&gt;&lt;/span&gt;
			&lt;/button&gt;
			&lt;ul class=&quot;dropdown-menu dropdown-menu-left&quot; role=&quot;menu&quot;&gt;
				&lt;li&gt;&lt;pre&gt;@article{Gibson:2018,
  author = {Gibson, Helen and Vickers, Paul},
  journal = {arXiv preprint arXiv:1712.05644},
  month = dec,
  pdf = {https://arxiv.org/pdf/1712.05644},
  title = {graphTPP: A multivariate based method for interactive graph layout and analysis},
  year = {2017}
}
&lt;/pre&gt;&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/div&gt;
	&lt;/div&gt;
	&lt;div class=&quot;col-md-7&quot;&gt;
			
				&lt;a role=&quot;button&quot; class=&quot;btn btn-primary btn-xs&quot; href=&quot;https://arxiv.org/pdf/1712.05644&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;
			
			
			
			
			
			
			
			
	&lt;/div&gt;
	&lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;/li&gt;&lt;/ul&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="graph layout" /><category term="targeted projection pursuit" /><category term="adjacency matrix" /><summary type="html">Graph layout is the process of creating a visual representation of a graph through a node-link diagram. Node-attribute graphs have additional data stored on the nodes which describe certain properties of the nodes called attributes.</summary></entry><entry><title type="html">Sonification for Structural Biology and Structure-based Drug Design</title><link href="https://paulvickers.github.io/chat/public/Sonification-for-Structural-Biology/" rel="alternate" type="text/html" title="Sonification for Structural Biology and Structure-based Drug Design" /><published>2017-12-06T00:00:00+00:00</published><updated>2017-12-06T00:00:00+00:00</updated><id>https://paulvickers.github.io/chat/public/Sonification%20for%20Structural%20Biology</id><content type="html" xml:base="https://paulvickers.github.io/chat/public/Sonification-for-Structural-Biology/">&lt;p&gt;The visualisation of structural biology data can be quite challenging as the datasets
are complex, in particular the intrinsic dynamics/flexibility. Therefore some
researchers have looked into the use of sonification for the display of proteins.
 You can find more about the project on the &lt;a href=&quot;/chison&quot;&gt;project page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">The visualisation of structural biology data can be quite challenging as the datasets are complex, in particular the intrinsic dynamics/flexibility. Therefore some researchers have looked into the use of sonification for the display of proteins.</summary></entry></feed>