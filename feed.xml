<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-04-09T10:30:56+01:00</updated><id>http://localhost:4000/</id><title type="html">Paul Vickers</title><subtitle>Sonification, auditory display, human-computer interaction, aesthetics, network security, visualization, digital living, computational perceptualization.</subtitle><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><entry><title type="html">Sonification of Network Traffic Flow for Monitoring and Situational Awareness</title><link href="http://localhost:4000/chat/public/SoNSTAR/" rel="alternate" type="text/html" title="Sonification of Network Traffic Flow for Monitoring and Situational Awareness" /><published>2017-12-20T00:00:00+00:00</published><updated>2017-12-20T00:00:00+00:00</updated><id>http://localhost:4000/chat/public/SoNSTAR</id><content type="html" xml:base="http://localhost:4000/chat/public/SoNSTAR/">&lt;p&gt;Maintaining situational awareness of what is happening within a computer network is
challenging, not least because the behaviour happens within computers and communications
networks, but also because data traffic speeds and volumes are beyond human ability to
process. Visualisation techniques are widely used to present information about the
dynamics of network traffic dynamics. Although they provide operators with an overall
view and specific information about particular traffic or attacks on the network, they
often still fail to represent the events in an understandable way. Also, visualisations
require visual attention and so are not well suited to continuous monitoring scenarios
in which network administrators must carry out other tasks. Situational awareness is
critical and essential for decision-making in the domain of computer network monitoring
where it is vital to be able to identify and recognize network environment
behaviours.Here we present SoNSTAR (Sonification of Networks for SiTuational AwaReness),
a real-time sonification system to be used in the monitoring of computer networks to
support the situational awareness of network administrators. SoNSTAR provides an
auditory representation of all the TCP/IP protocol traffic within a network based on the
different traffic flows between between network hosts. SoNSTAR narrows the gap between
network administrators and the cyber environment so they can more quickly recognise and
learn about the way the traffic flows within their network behave and change. SoNSTAR
raises situational awareness levels for computer network defence by allowing operators
to achieve better understanding and performance while imposing less workload compared to
visual techniques. SoNSTAR identifyies the features of network traffic flows by
inspecting the status flags of TCP/IP packet headers. Different combinations of these
features define particular traffic events and these these events are mapped to recorded
sounds to generate a soundscape that represents the real-time status of the network
traffic environment. Listening to the sequence, timing, and loudness of the different
sounds within the soundscape allows the network administrator to monitor the network and
recognise anomalous behaviour quickly and without having to continuously look at a
computer screen.&lt;/p&gt;

&lt;p&gt;A pre-print of the article is &lt;a href=&quot;http://arxiv.org/abs/1712.07029&quot;&gt;available on arXiv&lt;/a&gt;. It is 
part of the &lt;a href=&quot;/SoNSTAR/&quot;&gt;SoNSTAR&lt;/a&gt; project.&lt;/p&gt;

&lt;ul class=&quot;bibliography&quot; reversed=&quot;&quot;&gt;&lt;li&gt;&lt;span id=&quot;Debashi:2018&quot;&gt;Debashi, M., &amp;amp; Vickers, P. (2018). &lt;b&gt;Sonification of Network Traffic Flow for Monitoring and Situational Awareness&lt;/b&gt;. &lt;i&gt;PLoS One&lt;/i&gt;, &lt;i&gt;In press&lt;/i&gt;.&lt;/span&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-md-2&quot;&gt;
		&lt;div class=&quot;btn-group&quot;&gt;
			&lt;button type=&quot;button&quot; class=&quot;btn btn-info btn-xs dropdown-toggle&quot; data-toggle=&quot;dropdown&quot;&gt;
				Bib Entry
				&lt;span class=&quot;caret&quot;&gt;&lt;/span&gt;
			&lt;/button&gt;
			&lt;ul class=&quot;dropdown-menu dropdown-menu-left&quot; role=&quot;menu&quot;&gt;
				&lt;li&gt;&lt;pre&gt;@article{Debashi:2018,
  author = {Debashi, Mohamed and Vickers, Paul},
  doi = {10.1371/journal.pone.0195948},
  journal = {PLoS One},
  title = {Sonification of Network Traffic Flow for Monitoring and Situational Awareness},
  volume = {In press},
  year = {2018}
}
&lt;/pre&gt;&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/div&gt;
	&lt;/div&gt;
	&lt;div class=&quot;col-md-7&quot;&gt;
			
			
			
			
			
			
				&lt;a role=&quot;button&quot; class=&quot;btn btn-warning btn-xs&quot; href=&quot;https://doi.org/10.1371/journal.pone.0195948&quot; target=&quot;_blank&quot;&gt;doi&lt;/a&gt;
			
			
			
	&lt;/div&gt;
	&lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;/li&gt;&lt;/ul&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="network monitoring" /><category term="situational awareness" /><summary type="html">Maintaining situational awareness of what is happening within a computer network is challenging, not least because the behaviour happens within computers and communications networks, but also because data traffic speeds and volumes are beyond human ability to process.</summary></entry><entry><title type="html">graphTPP: A multivariate based method for interactive graph layout and analysis</title><link href="http://localhost:4000/chat/public/graphTPP/" rel="alternate" type="text/html" title="graphTPP: A multivariate based method for interactive graph layout and analysis" /><published>2017-12-18T00:00:00+00:00</published><updated>2017-12-18T00:00:00+00:00</updated><id>http://localhost:4000/chat/public/graphTPP</id><content type="html" xml:base="http://localhost:4000/chat/public/graphTPP/">&lt;p&gt;Graph layout is the process of creating a visual representation of a graph through a
node-link diagram. Node-attribute graphs have additional data stored on the nodes which
describe certain properties of the nodes called attributes. Typical force-directed
representations often produce hairball-like structures that neither aid in understanding
the graph’s topology nor the relationship to its attributes. The aim of this research
was to investigate the use of node-attributes for graph layout in order to improve the
analysis process and to give further insight into the graph over purely topological
layouts. In this article we present graphTPP, a graph based extension to targeted
projection pursuit (TPP) — an interactive, linear, dimension reduction technique — as a
method for graph layout and subsequent further analysis. TPP allows users to control the
projection and is optimised for clustering. Three case studies were conducted in the
areas of influence graphs, network security, and citation networks. In each case
graphTPP was shown to outperform standard force-directed techniques and even other
dimension reduction methods in terms of clarity of clustered structure in the layout,
the association between the structure and the attributes and the insights elicited in
each domain area. A pre-print of the article is &lt;a href=&quot;http://arxiv.org/abs/1712.05644&quot;&gt;available on arXiv&lt;/a&gt;.&lt;/p&gt;

&lt;ul class=&quot;bibliography&quot; reversed=&quot;&quot;&gt;&lt;li&gt;&lt;span id=&quot;Gibson:2018&quot;&gt;Gibson, H., &amp;amp; Vickers, P. (2017). &lt;b&gt;graphTPP: A multivariate based method for interactive graph layout and analysis&lt;/b&gt;. &lt;i&gt;ArXiv Preprint ArXiv:1712.05644&lt;/i&gt;.&lt;/span&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-md-2&quot;&gt;
		&lt;div class=&quot;btn-group&quot;&gt;
			&lt;button type=&quot;button&quot; class=&quot;btn btn-info btn-xs dropdown-toggle&quot; data-toggle=&quot;dropdown&quot;&gt;
				Bib Entry
				&lt;span class=&quot;caret&quot;&gt;&lt;/span&gt;
			&lt;/button&gt;
			&lt;ul class=&quot;dropdown-menu dropdown-menu-left&quot; role=&quot;menu&quot;&gt;
				&lt;li&gt;&lt;pre&gt;@article{Gibson:2018,
  author = {Gibson, Helen and Vickers, Paul},
  journal = {arXiv preprint arXiv:1712.05644},
  month = dec,
  pdf = {https://arxiv.org/pdf/1712.05644},
  title = {graphTPP: A multivariate based method for interactive graph layout and analysis},
  url = {http://arxiv.org/abs/1712.05644},
  year = {2017}
}
&lt;/pre&gt;&lt;/li&gt;
			&lt;/ul&gt;
		&lt;/div&gt;
	&lt;/div&gt;
	&lt;div class=&quot;col-md-7&quot;&gt;
			
				&lt;a role=&quot;button&quot; class=&quot;btn btn-primary btn-xs&quot; href=&quot;https://arxiv.org/pdf/1712.05644&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;
			
			
			
			
			
			
			
				&lt;a role=&quot;button&quot; class=&quot;btn btn-warning btn-xs&quot; href=&quot;http://arxiv.org/abs/1712.05644&quot; target=&quot;_blank&quot;&gt;web&lt;/a&gt;
			
			
	&lt;/div&gt;
	&lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;/li&gt;&lt;/ul&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="graph layout" /><category term="targeted projection pursuit" /><category term="adjacency matrix" /><summary type="html">Graph layout is the process of creating a visual representation of a graph through a node-link diagram. Node-attribute graphs have additional data stored on the nodes which describe certain properties of the nodes called attributes.</summary></entry><entry><title type="html">Sonification for Structural Biology and Structure-based Drug Design</title><link href="http://localhost:4000/chat/public/Sonification-for-Structural-Biology/" rel="alternate" type="text/html" title="Sonification for Structural Biology and Structure-based Drug Design" /><published>2017-12-06T00:00:00+00:00</published><updated>2017-12-06T00:00:00+00:00</updated><id>http://localhost:4000/chat/public/Sonification%20for%20Structural%20Biology</id><content type="html" xml:base="http://localhost:4000/chat/public/Sonification-for-Structural-Biology/">&lt;p&gt;The visualisation of structural biology data can be quite challenging as the datasets
are complex, in particular the intrinsic dynamics/flexibility. Therefore some
researchers have looked into the use of sonification for the display of proteins.
 You can find more about the project on the &lt;a href=&quot;/chison&quot;&gt;project page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">The visualisation of structural biology data can be quite challenging as the datasets are complex, in particular the intrinsic dynamics/flexibility. Therefore some researchers have looked into the use of sonification for the display of proteins.</summary></entry><entry><title type="html">DSSon pre-print</title><link href="http://localhost:4000/chat/public/Direct-segmented-sonification-preprint/" rel="alternate" type="text/html" title="DSSon pre-print" /><published>2017-12-05T00:00:00+00:00</published><updated>2017-12-05T00:00:00+00:00</updated><id>http://localhost:4000/chat/public/Direct-segmented-sonification-preprint</id><content type="html" xml:base="http://localhost:4000/chat/public/Direct-segmented-sonification-preprint/">&lt;p&gt;I have posted a &lt;a href=&quot;http://arxiv.org/abs/1711.11368&quot;&gt;pre-print on arXiv&lt;/a&gt; of the work Robert Höldrich and I developed on
direct segmented sonification. You can find more about the project on the &lt;a href=&quot;/DSSon&quot;&gt;DSSon page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">I have posted a [pre-print on arXiv](http://arxiv.org/abs/1711.11368) of our direct segmented sonification project.</summary></entry><entry><title type="html">Sonification Repositories</title><link href="http://localhost:4000/chat/public/Sonification-repositories/" rel="alternate" type="text/html" title="Sonification Repositories" /><published>2017-12-01T00:00:00+00:00</published><updated>2017-12-01T00:00:00+00:00</updated><id>http://localhost:4000/chat/public/Sonification-repositories</id><content type="html" xml:base="http://localhost:4000/chat/public/Sonification-repositories/">&lt;p&gt;This site provides links to our sonification research project repositories on Github. These repos
contain source code, data sets, and example sonifications and each has a DOI to make it
citable. The ones available so far are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/DSSon&quot;&gt;nuson-DSSon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/alchemical_sensing/&quot;&gt;nuson-moson&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/socs/&quot;&gt;nuson-socs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/SoNSTAR&quot;&gt;nuson-SoNSTAR&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">This site provides links to our sonification research project repositories.</summary></entry><entry><title type="html">DSSon: Direct Segmented Sonification</title><link href="http://localhost:4000/chat/public/Direct-Segmented-Sonification/" rel="alternate" type="text/html" title="DSSon: Direct Segmented Sonification" /><published>2017-10-05T00:00:00+01:00</published><updated>2017-10-05T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Direct-Segmented-Sonification</id><content type="html" xml:base="http://localhost:4000/chat/public/Direct-Segmented-Sonification/">&lt;p&gt;Following a research visit I made in the spring of this year to the 
&lt;a href=&quot;http://iem.kug.ac.at/en/institute-of-electronic-music-and-acoustics.html&quot;&gt;Institute of Electronic Music and Acoustics&lt;/a&gt;
 at the Kunst Uni Graz, &lt;a href=&quot;http://iem.kug.ac.at/en/people.html?tx_kugpeople_pi1%5Bperson_nr%5D=50114&amp;amp;cHash=eb4d7486e953326e239071165ea47ccf&quot;&gt;Robert Höldrich&lt;/a&gt;
 and I have been collaborating on a new sonification technique
that picks out regions of interest in a signal while preserving the original temporal regime. Called
Direct Segmented Sonification, it leverages the directness of audification while using
a modified auditory graphing approach to sonify segments of the original data signal
while preserving the original temporal regime. Read more on the &lt;a href=&quot;/DSSon&quot;&gt;DSSon project page&lt;/a&gt;. From there you will find a link to the project’s 
github repository which contains source code and audio examples. A paper on the technique 
is currently under review.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">Robert Höldrich and I have been collaborating on a new sonification technique that picks out regions of interest in a signal while preserving the original temporal regime.</summary></entry><entry><title type="html">soniFRED: Sonification of Biomechanical Exercise Data</title><link href="http://localhost:4000/chat/public/Sonification-of-FRED-exercise/" rel="alternate" type="text/html" title="soniFRED: Sonification of Biomechanical Exercise Data" /><published>2017-06-02T00:00:00+01:00</published><updated>2017-06-02T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Sonification-of-FRED-exercise</id><content type="html" xml:base="http://localhost:4000/chat/public/Sonification-of-FRED-exercise/">&lt;p&gt;The &lt;a href=&quot;https://paulvickers.github.io/SoniFRED/&quot;&gt;soniFRED project&lt;/a&gt; is a piece of 
collaborative research between Northumbria University’s Department of Computer and 
Information Sciences and the &lt;a href=&quot;http://www.aerospacemed.rehab/&quot;&gt;Aerospace Medicine and Rehabilitation Lab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We spent a week at the &lt;a href=&quot;https://www.life.org.uk/&quot;&gt;Centre for Life&lt;/a&gt;
testing different information presentation methods to help people with low back pain
exercise most effectively with the Functional Re-adaptive Exercise Device (FRED). Combinations
of visual and auditory feedback (including two sonification schemes) were given to 
participants and the data will be analysed to see what ways of giving the users feedback
about how they are using the machine lead to improved performance.&lt;/p&gt;

&lt;p&gt;You can get a sense of what we were doing by watching this &lt;a href=&quot;https://vimeo.com/219668024&quot;&gt;short video&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">We spent a week at the [Centre for Life](https://www.life.org.uk/) testing different information presentation methods to help people with low back pain exercise most effectively.</summary></entry><entry><title type="html">Displays: Special issue on the sonification of real-time data</title><link href="http://localhost:4000/chat/public/Special-issue/" rel="alternate" type="text/html" title="Displays: Special issue on the sonification of real-time data" /><published>2017-04-14T00:00:00+01:00</published><updated>2017-04-14T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Special-issue</id><content type="html" xml:base="http://localhost:4000/chat/public/Special-issue/">&lt;p&gt;David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the 
sonification of real-time data. This special issue presents five articles drawn from
across the range of sonification practice but which all focus on how to communicate
information about real-time data.&lt;/p&gt;

&lt;p&gt;The contents are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216302244&quot;&gt;Preface to the Special Issue on Sonification&lt;/a&gt;” by Paul Vickers, David Worrall, and Richard So.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300634&quot;&gt;Comparative study on the effect of Parameter Mapping Sonification on perceived instabilities, efficiency, and accuracy in real-time interactive exploration of noisy data streams&lt;/a&gt;” by David Poirier-Quinot, Gaetan Parseihian, and Brian F.G. Katz.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300646&quot;&gt;Sonification of a network’s self-organized criticality for real-time situational awareness&lt;/a&gt;” by Paul Vickers, Chris Laing, and Tom Fairfax.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216300828&quot;&gt;Towards a systematic approach to real-time sonification design for surface electromyography&lt;/a&gt;” by S. Camille Peres, Daniel Verona, Tariq Nisar, and Paul Ritchey.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301652&quot;&gt;The sound of smile: Auditory biofeedback of facial EMG activity&lt;/a&gt;” by Yuki Nakayama, Yuji Takano, Masaki Matsubara, Kenji Suzuki, and Hiroko Terasawa.&lt;/li&gt;
  &lt;li&gt;“&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0141938216301949&quot;&gt;EcoSonic: Auditory peripheral monitoring of fuel consumption for fuel-efficient driving&lt;/a&gt;” by Jan Hammerschmidt and Thomas Hermann.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can see the full issue at &lt;a href=&quot;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&quot;&gt;http://www.sciencedirect.com/science/journal/01419382/47/supp/C&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><summary type="html">David Worrall and I guest-edited the April 2017 special isse of the journal Displays on the sonification of real-time data.</summary></entry><entry><title type="html">Body, Sound and Space in Music and Beyond: Multimodal Explorations</title><link href="http://localhost:4000/chat/public/Body-Sound-and-Space-in-Music-and-Beyond/" rel="alternate" type="text/html" title="Body, Sound and Space in Music and Beyond: Multimodal Explorations" /><published>2017-04-13T00:00:00+01:00</published><updated>2017-04-13T00:00:00+01:00</updated><id>http://localhost:4000/chat/public/Body-Sound-and-Space-in-Music-and-Beyond</id><content type="html" xml:base="http://localhost:4000/chat/public/Body-Sound-and-Space-in-Music-and-Beyond/">&lt;p&gt;Clemens Wollner has edited a terrific volume entitled &lt;a href=&quot;https://www.routledge.com/Body-Sound-and-Space-in-Music-and-Beyond-Multimodal-Explorations/Wollner/p/book/9781472485403&quot;&gt;&lt;em&gt;Body, Sound and Space in Music and Beyond: Multimodal Explorations&lt;/em&gt;&lt;/a&gt;
which has been published by Routledge. With Bennett Hogg and David Worrall, I co-wrote Chapter 6,
&lt;em&gt;Aesthetics of sonification: Taking the subject-position&lt;/em&gt; which takes forward my
work on sonification aesthetics and proposes a way of considering sonification design that
may help to bring the listener’s past experience into the listening experience.&lt;/p&gt;

&lt;p&gt;Here’s the blurb about the book from its website:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Body and space refer to vital and interrelated dimensions in the experience of sounds
and music. Sounds have an overwhelming impact on feelings of bodily presence and 
inform us about the space we experience. Even in situations where visual information 
is artificial or blurred, such as in virtual environments or certain genres of film 
and computer games, sounds may shape our perceptions and lead to surprising new 
experiences. This book discusses recent developments in a range of interdisciplinary 
fields, taking into account the rapidly changing ways of experiencing sounds and 
music, the consequences for how we engage with sonic events in daily life and the 
technological advancements that offer insights into state-of-the-art methods and 
future perspectives. Topics range from the pleasures of being locked into the beat 
of the music, perception–action coupling and bodily resonance, and affordances of 
musical instruments, to neural processing and cross-modal experiences of space and 
pitch. Applications of these findings are discussed for movement sonification, room 
acoustics, networked performance, and for the spatial coordination of movements in 
dance, computer gaming and interactive artistic installations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can find details about the book &lt;a href=&quot;https://www.routledge.com/Body-Sound-and-Space-in-Music-and-Beyond-Multimodal-Explorations/Wollner/p/book/9781472485403&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="music" /><summary type="html">Body and space refer to vital and interrelated dimensions in the experience of sounds and music. Sounds have an overwhelming impact on feelings of bodily presence and inform us about the space we experience.</summary></entry><entry><title type="html">Audiogram: Embed audio in social media!</title><link href="http://localhost:4000/chat/public/Audiogram-embed-audio-in-social-media-copy/" rel="alternate" type="text/html" title="Audiogram: Embed audio in social media!" /><published>2017-01-19T00:00:00+00:00</published><updated>2017-01-19T00:00:00+00:00</updated><id>http://localhost:4000/chat/public/Audiogram-embed-audio-in-social-media%20copy</id><content type="html" xml:base="http://localhost:4000/chat/public/Audiogram-embed-audio-in-social-media-copy/">&lt;p&gt;We know as sonification researchers that social media platforms prefer video over audio. This
is frustrating as we want to be able, say, to embed sonification examples in our Tweets.&lt;/p&gt;

&lt;p&gt;Well, now we can thanks to New York Public radio who have created an open source tool to 
convert audio into video which can be embedded in social media. The tool is called
&lt;a href=&quot;https://medium.com/@WNYC/socialaudio-e648e8a5f2e9#.xi3wf2nst&quot;&gt;Audiogram&lt;/a&gt; and you can get it from their &lt;a href=&quot;https://github.com/nypublicradio/audiogram&quot;&gt;git repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/DrPaulVickers/status/822066017283215361&quot;&gt;Here’s a Tweet&lt;/a&gt; with it in action.&lt;/p&gt;</content><author><name>Dr Paul Vickers</name><email>paul.vickers@northumbria.ac.uk</email></author><category term="sonification" /><category term="tools" /><summary type="html">New York Public radio have created an open source tool to convert audio into video which can be embedded in social media.</summary></entry></feed>